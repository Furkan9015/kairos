{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8f47f0-1477-40a6-b091-6abbfb9866f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append('/hpc/compgen/users/mpages/babe/src')\n",
    "sys.path.append('/hpc/compgen/users/mpages/babe/models')\n",
    "\n",
    "from gridmodel import GridAnalysisModel\n",
    "from normalization import normalize_signal_from_read_data, med_mad\n",
    "from read import read_fast5, list_reads_ids\n",
    "import uuid\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d250432",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f095d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFast5Dataset(Dataset):\n",
    "    \"\"\"Base dataset class that iterates over fast5 files for basecalling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        data_dir = None, \n",
    "        fast5_list = None, \n",
    "        recursive = True, \n",
    "        buffer_size = 100,\n",
    "        window_size = 2000,\n",
    "        window_overlap = 400,\n",
    "        trim_signal = True,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): dir where the fast5 file\n",
    "            fast5_list (str): file with a list of files to be processed\n",
    "            recursive (bool): if the data_dir should be searched recursively\n",
    "            buffer_size (int): number of fast5 files to read \n",
    "\n",
    "        data_dir and fast5_list are esclusive\n",
    "        \"\"\"\n",
    "        \n",
    "        super(BaseFast5Dataset, self).__init__()\n",
    "    \n",
    "        self.data_dir = data_dir\n",
    "        self.recursive = recursive\n",
    "        self.buffer_size = buffer_size\n",
    "        self.window_size = window_size\n",
    "        self.window_overlap = window_overlap\n",
    "        self.trim_signal = trim_signal\n",
    "\n",
    "        if fast5_list is None:\n",
    "            self.data_files = self.find_all_fast5_files()\n",
    "        else:\n",
    "            self.data_files = self.read_fast5_list(fast5_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.process_reads(self.data_files[idx])\n",
    "\n",
    "        \n",
    "    def find_all_fast5_files(self):\n",
    "        \"\"\"Find all fast5 files in a dir recursively\n",
    "        \"\"\"\n",
    "        # find all the files that we have to process\n",
    "        files_list = list()\n",
    "        for path in Path(self.data_dir).rglob('*.fast5'):\n",
    "            files_list.append(str(path))\n",
    "        files_list = self.buffer_list(files_list, self.buffer_size)\n",
    "        return files_list\n",
    "\n",
    "    def read_fast5_list(self, fast5_list):\n",
    "        \"\"\"Read a text file with the reads to be processed\n",
    "        \"\"\"\n",
    "\n",
    "        files_list = list()\n",
    "        with open(fast5_list, 'r') as f:\n",
    "            for line in f:\n",
    "                files_list.append(line.strip('\\n'))\n",
    "        files_list = self.buffer_list(files_list, self.buffer_size)\n",
    "        return files_list\n",
    "\n",
    "    def buffer_list(self, files_list, buffer_size):\n",
    "        buffered_list = list()\n",
    "        for i in range(0, len(files_list), buffer_size):\n",
    "            buffered_list.append(files_list[i:i+buffer_size])\n",
    "        return buffered_list\n",
    "\n",
    "    def trim(self, signal, window_size=40, threshold_factor=2.4, min_elements=3):\n",
    "        \"\"\"\n",
    "\n",
    "        from: https://github.com/nanoporetech/bonito/blob/master/bonito/fast5.py\n",
    "        \"\"\"\n",
    "\n",
    "        min_trim = 10\n",
    "        signal = signal[min_trim:]\n",
    "\n",
    "        med, mad = med_mad(signal[-(window_size*100):])\n",
    "\n",
    "        threshold = med + mad * threshold_factor\n",
    "        num_windows = len(signal) // window_size\n",
    "\n",
    "        seen_peak = False\n",
    "\n",
    "        for pos in range(num_windows):\n",
    "            start = pos * window_size\n",
    "            end = start + window_size\n",
    "            window = signal[start:end]\n",
    "            if len(window[window > threshold]) > min_elements or seen_peak:\n",
    "                seen_peak = True\n",
    "                if window[-1] > threshold:\n",
    "                    continue\n",
    "                return min(end + min_trim, len(signal)), len(signal)\n",
    "\n",
    "        return min_trim, len(signal)\n",
    "\n",
    "    def chunk(self, signal, chunksize, overlap):\n",
    "        \"\"\"\n",
    "        Convert a read into overlapping chunks before calling\n",
    "\n",
    "        The first N datapoints will be cut out so that the window ends perfectly\n",
    "        with the number of datapoints of the read.\n",
    "        \"\"\"\n",
    "        if isinstance(signal, np.ndarray):\n",
    "            signal = torch.from_numpy(signal)\n",
    "\n",
    "        T = signal.shape[0]\n",
    "        if chunksize == 0:\n",
    "            chunks = signal[None, :]\n",
    "        elif T < chunksize:\n",
    "            chunks = torch.nn.functional.pad(signal, (chunksize - T, 0))[None, :]\n",
    "        else:\n",
    "            stub = (T - overlap) % (chunksize - overlap)\n",
    "            chunks = signal[stub:].unfold(0, chunksize, chunksize - overlap)\n",
    "        \n",
    "        return chunks.unsqueeze(1)\n",
    "    \n",
    "    def normalize(self, read_data):\n",
    "        return normalize_signal_from_read_data(read_data)\n",
    "\n",
    "    def process_reads(self, read_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            read_list (list): list of files to be processed\n",
    "\n",
    "        Returns:\n",
    "            two arrays, the first one with the normalzized chunked data,\n",
    "            the second one with the read ids of each chunk.\n",
    "        \"\"\"\n",
    "        chunks_list = list()\n",
    "        id_list = list()\n",
    "        l_list = list()\n",
    "\n",
    "        for read_file in read_list:\n",
    "            reads_data = read_fast5(read_file)\n",
    "\n",
    "            for read_id in reads_data.keys():\n",
    "                read_data = reads_data[read_id]\n",
    "                norm_signal = self.normalize(read_data)\n",
    "\n",
    "                if self.trim_signal:\n",
    "                    trim, _ = self.trim(norm_signal[:8000])\n",
    "                    norm_signal = norm_signal[trim:]\n",
    "\n",
    "                chunks = self.chunk(norm_signal, self.window_size, self.window_overlap)\n",
    "                num_chunks = chunks.shape[0]\n",
    "                \n",
    "                uuid_fields = uuid.UUID(read_id).fields\n",
    "                id_arr = np.zeros((num_chunks, 6), dtype = np.int)\n",
    "                for i, uf in enumerate(uuid_fields):\n",
    "                    id_arr[:, i] = uf\n",
    "                \n",
    "                id_list.append(id_arr)\n",
    "                l_list.append(np.full((num_chunks,), len(norm_signal)))\n",
    "                chunks_list.append(chunks)\n",
    "        \n",
    "        out = {\n",
    "            'x': torch.vstack(chunks_list).squeeze(1), \n",
    "            'id': np.vstack(id_list),\n",
    "            'len': np.concatenate(l_list)\n",
    "        }\n",
    "        return out\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b675e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = '/hpc/compgen/users/mpages/babe/doc/splits/human_task_test_reads.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509b7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = BaseFast5Dataset(fast5_list= files_list, buffer_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f7c7a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee83524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/hpc/compgen/projects/nanoxog/babe/analysis/mpages/models/grid_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53cd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'human'\n",
    "model_name = 'bonito_bonitorev_ctc_True_2000'\n",
    "\n",
    "config = model_name.split('_')\n",
    "\n",
    "cnn_type = config[0]\n",
    "encoder_type = config[1]\n",
    "decoder_type = config[2]\n",
    "use_connector = config[3]\n",
    "window_size = config[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87ff532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "log = df = pd.read_csv(os.path.join(model_dir, task, model_name, 'train.log'))\n",
    "log = log[log['checkpoint'] == 'yes']\n",
    "best_step = log['step'].iloc[np.argmax(log['metric.accuracy.val'])]\n",
    "checkpoint_file = os.path.join(model_dir, task, model_name, 'checkpoints', 'checkpoint_' + str(best_step) + '.pt')\n",
    "\n",
    "use_amp = True\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "model = GridAnalysisModel(\n",
    "    cnn_type = cnn_type, \n",
    "    encoder_type = encoder_type, \n",
    "    decoder_type = decoder_type,\n",
    "    use_connector = use_connector,\n",
    "    device = device,\n",
    "    dataloader_train = None, \n",
    "    dataloader_validation = None, \n",
    "    scaler = scaler,\n",
    "    use_amp = use_amp\n",
    ")\n",
    "\n",
    "state_dict = torch.load(checkpoint_file)\n",
    "model.load_state_dict(state_dict['model_state'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1387824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBasecaller:\n",
    "    \"\"\"A base Basecaller class that is used to basecall complete reads\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, model, batch_size, n_cores = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): a model that has the following methods:\n",
    "                predict, decode\n",
    "            chunk_size (int): length of the chunks that a read will be divided into\n",
    "            overlap (int): amount of overlap between consecutive chunks\n",
    "            batch_size (int): batch size to forward through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset = DataLoader(dataset, batch_size=1, shuffle=False, num_workers = n_cores)\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def basecall(self, verbose = True):\n",
    "\n",
    "        # iterate over the data\n",
    "        for batch in tqdm(self.dataset, disable = not verbose):\n",
    "            \n",
    "            x = batch['x']\n",
    "            l = x.shape[0]\n",
    "            ss = torch.arange(0, l, self.batch_size)\n",
    "            nn = ss + self.batch_size\n",
    "\n",
    "            p_list = list()\n",
    "            for s, n in zip(ss, nn):\n",
    "                p = self.model.predict_step(x[s:n, :])\n",
    "                p_list.append(p)\n",
    "\n",
    "            return p_list\n",
    "    \n",
    "    \n",
    "    def stich(self, chunks, method, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Stitch chunks together with a given overlap\n",
    "        \n",
    "        Args:\n",
    "            chunks (tensor): predictions with shape [samples, length, classes]\n",
    "        \"\"\"\n",
    "\n",
    "        if method == 'stride':\n",
    "            return self.stich_by_stride(chunks, *args, **kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def stitch_by_stride(self, chunks, chunksize, overlap, length, stride, reverse=False):\n",
    "        \"\"\"\n",
    "        Stitch chunks together with a given overlap\n",
    "        \n",
    "        This works by calculating what the overlap should be between two outputed\n",
    "        chunks from the network based on the stride and overlap of the inital chunks.\n",
    "        The overlap section is divided in half and the outer parts of the overlap\n",
    "        are discarded and the chunks are concatenated. There is no alignment.\n",
    "        \n",
    "        Chunk1: AAAAAAAAAAAAAABBBBBCCCCC\n",
    "        Chunk2:               DDDDDEEEEEFFFFFFFFFFFFFF\n",
    "        Result: AAAAAAAAAAAAAABBBBBEEEEEFFFFFFFFFFFFFF\n",
    "        \n",
    "        Args:\n",
    "            chunks (tensor): predictions with shape [samples, length, classes]\n",
    "            chunk_size (int): initial size of the chunks\n",
    "            overlap (int): initial overlap of the chunks\n",
    "            length (int): original length of the signal\n",
    "            stride (int): stride of the model\n",
    "            reverse (bool): if the chunks are in reverse order\n",
    "            \n",
    "        Copied from https://github.com/nanoporetech/bonito\n",
    "        \"\"\"\n",
    "        if chunks.shape[0] == 1: return chunks.squeeze(0)\n",
    "\n",
    "        semi_overlap = overlap // 2\n",
    "        start, end = semi_overlap // stride, (chunksize - semi_overlap) // stride\n",
    "        stub = (length - overlap) % (chunksize - overlap)\n",
    "        first_chunk_end = (stub + semi_overlap) // stride if (stub > 0) else end\n",
    "\n",
    "        if reverse:\n",
    "            chunks = list(chunks)\n",
    "            return torch.cat([\n",
    "                chunks[-1][:-start], *(x[-end:-start] for x in reversed(chunks[1:-1])), chunks[0][-first_chunk_end:]\n",
    "            ])\n",
    "        else:\n",
    "            return torch.cat([\n",
    "                chunks[0, :first_chunk_end], *chunks[1:-1, start:end], chunks[-1, start:]\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8423ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BB = BaseBasecaller(DS, model, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3924818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                              | 0/2500 [00:23<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(BB.dataset, disable = not True):\n",
    "    \n",
    "    x = batch['x'].squeeze(0)\n",
    "    l = x.shape[0]\n",
    "    ss = torch.arange(0, l, BB.batch_size)\n",
    "    nn = ss + BB.batch_size\n",
    "\n",
    "    p_list = list()\n",
    "    for s, n in zip(ss, nn):\n",
    "        p = BB.model.predict_step({'x':x[s:n, :]})\n",
    "        p_list.append(p)\n",
    "        \n",
    "    p = torch.hstack(p_list)\n",
    "\n",
    "    ids = batch['id'][0]\n",
    "    ids_arr = np.zeros((ids.shape[0], ), dtype = 'U32')\n",
    "    for i in range(ids.shape[0]):\n",
    "        ids_arr[i] = str(uuid.UUID(fields=ids[i].tolist()))\n",
    "\n",
    "    read_stacks = dict()\n",
    "    read_lens = dict()\n",
    "    for id in np.unique(ids_arr):\n",
    "        w = np.where(ids_arr == id)[0]\n",
    "        read_stacks[id] = p[:, w, :].permute(1, 0, 2).exp()\n",
    "        read_lens[id] = batch['len'][0, w[0]].item()\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db781da",
   "metadata": {},
   "outputs": [],
   "source": [
    "kid = '4afc223a-939c-4e72-afe7-45801f55'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57ad6de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 400, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_stacks[kid].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd8dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stiched_p = BB.stitch_by_stride(read_stacks[kid], chunksize = 2000, overlap = 400, length = read_lens[kid], stride = 5, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e311c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_ctc_decode import viterbi_search\n",
    "\n",
    "seq, path = viterbi_search(stiched_p.cpu().numpy(), 'NACGT', qstring = True, qscale = 1.0, qbias = 0.0, collapse_repeats = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b0dc73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = model.decode(stiched_p.unsqueeze(1).cpu().numpy(), greedy = True, qstring = True, collapse_repeats = True, return_path = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80cf4d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2852"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a833de9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TAATGGTCCCTTGGATAAATATCCTCAGATTGCTTTTGGGAAAGTTTCCCTAACCACTGCTGTTTACCTGTTTGCTACACTGCAAATCCATTAATTTTTAGTTTTGAAAATGCAGTTTTCTTGAAACCCTGATGCTTCAATCATTCAGCGATCTAGTTTAGACAAGGCCTTCCAAATGGGGGAGGGGCAAGCTGCGGCGGCTTCGGGCTGGATTTGGGGGCAGGAGGGTTAAAGTACCTGAGGGATAAAGCAGTGGTGACATTAATGATACTTTTGGATAAAAAAGAAAAGCCAAGAACAAATACGTAGGGGTTCCCACTCCCTTTTCTTTGATTCCATCAGGGTAGAAACACAGCTCTGTGTGCCTTTGAGATCTTTCACTGGTGCTTCTTTACCCTTTGCTCATCAAGGGCTGACCATGTTTTGTGGGTTTTTGTTCATTTAGATAATCAATCTAAGATTATCTCAGGGCAATGGGATACCCAAGCAATGGTGGTATATGTGCCTATAACTTGATACTGGTGGGTGGGGGTCTGGTAACCCTAACTTGTCCTCTAGCAAGACCTCTGATGGGTTTCATGCTCCCAGAAGGTTTAGTCTCTAGCTTTTCTAGATTGCTTCCAATTCAAACCAACCCCCGTATCTTCTTGCTTTCCAACAGGGCACTCCTAAGCACCCTCTAGAGACTCCTCAATAGAGAAGATTAAAGAATGAGAAATTCAAAAGTTGAAAAAAACTCAAAAGGAATTAGGATGCAGAAGAAGAAAGAAAAAATATTTACTACTAAGCTTTGAATTCTCAATGTCAGCCCTGAAAAGTTATTGCCAGCAAAAAAAAAAGCCCCTTTCTAAGAGACATAAATTTTTGCAAACCCCTATTGCATTTTGATGGGATTGAGAGGCAGACTGTGGGACCAAAGAAGTATTTACTAATCAGTCCCAAATGGGGGGCAGTTCCAACTTTCCCTTGGGGTATGTTTTTCTCTCAATTCTCCTCAATCCCTCTGACTCCAAGAGTACCTGACTCGTTCTGCAACAACTCAACCTGCCCGGGATGCAGCATATAGTAAATGCCTCAGTGACTTCGGTCATCTATTGCAAATGGTTCAGGGATGCAGGAGTTTGGGAGGAGAGCTGCCTTGCAGTGAGCATGATGTGTCATTTTGTTGGAGAACATGGGTGCCTCTTGGACATATTCAAAGAAGACATTTAATTTCTGTAAGTTTCTTTTTCTCTGCCTTTTAAACTCCAAACCAGCTGTGCTGGTTCCATGAACTATAAAAAACATGAAAAAAAAAAAACAAATCCATACAAAAAAAGACAAACCTTAAAAAAACCAACCCAAACCAGCACAACAACCCAACCAACCAACCGGCAACCCCATATCCCTTTGTGTTTTTAATTTTAAGTAACTGAATTTCCTTAACGTTTTAAAGGGCATCTTTTCTCCTACTGCGAGGTCTATTATATAGAAACATAGATTTCTAGGGCATTCACTGCCTTGTAATAGGTAATTCTACTCATCAATTGTGTCCTGCTTAACATAAGAAAAACTTACTACTATTTTTAACATATTATTACTTTATCTACTCACTATGCCTTTTAGTAAACCATTCTCATTCACACAACGGGAAAAGTGGGTCATCAATTAAAGTCATCCAGTGTTTTCATGGCTACAAGTGGCCTGAGTGAATCGTCAAACTAGGAATTAGAACACTAATTCTGAATTTGAATACAATGCTCTGACTAAATCCACCCAATGGTCTTTGGCTATCTTATTTCCTGAGGCCAGGGATTCATAGCAAGTGTGTATCTGGGCAGGGGGGTAGGGTGGAGCAAAGTGGTAGGCTGGTTTGGAGAGCTAAGCTGAAAAACTAAAAAATGGAAAGACCTAAGAATATGAGTCTAAGGCTATCTGTGCTATTTGATCACAGGGTATGTCATTTTAACTGCTCTGTAACTACCAGGACTGCTTTCATAAGTAGCAGACTGCATTCTGCTCTGTGTCATCATTTGACAAATGTCTCCCATCCCATCCCCCCTTTACCTTTCCCTAGCACTCAGTTCCAAGGGGAGGAATGCTGAATTTGGAGTCAGATAAGGTTGGTTCCAGTGTTTGTATAGCAACTTGGACAAATCCTTGAGAACATAGATTTCCTTCTCATAGAAACAAATAATATTCTCTCTCCCTCACAGGGTTATTGCTTGAGGTTAAATGATATCTCTCGGCAGTCTTCTCCCAGCATCAAGGCATATAGTAAGCACTCCATAACATTGAATGAATAAATGAATGGCCTCAAGTCAGGCCCTTTGGGTGCTATGTTAATTCTTTCAGCCCCACTAGGATACTGGTGGAGTTCAGCACCTTTCCAAGAGCAGTGACTTTCTGAATAGTATTCTAATGGTCCCCCAAAGGCAGAGAATTTCATGAATGTCACTAGCGGTTATACCTGTTCTGGGAATACAGTGTTTCATTTTCCAGTGCTTATCAATCTATTACCTTTCACACCAAACACAAAATTCGGCTTTAAGAAGGGACAGAAACCAAGTGCCTTTCTGTTTATATAGGGTTGGGAGATATATTATTAAAAGTGAAACCGACTTCTCCTCTCTGATTTCTTGAAGGCAAAAAATCAGATTAGGAAATAAACTTTAGCTATATTTAGCTTTGCAGACCCTGGATAACGATGCCCTTGGGGGAAAAGGGAGAGTGTTAAGATCATTCAAACTATGGTTAGAACATTTCGTATATTTACTGCTTGTGCTTATGCTCTCTGCCTTAGTTTTACTCTCTAAGCCTTGAAGAATTCACAATTTACCCCAAACACAGTTTAATACATACGGGTGGGAATA'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0][0][:2852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4c86495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"&'&*&)&)'*+-%(%$(,+.*1./.0(-124&((62*1655:6&54++3'/40130.'&%(%(+%%.+0,04)(0+(/1-*00(5033%-32''(+,,++(141/,%2402.41'0576)11&674&,355..'371*3('$(./&&'$(%('++(-0.*'*1+%(&+%%&'**5-4.&)(%-(*)&-*/,750*-+.%.&'(&&)(),37.46,0250,(*)31'0-12&%&&%'4+77)'')+()04).(+-74-2,-28,745*)+)367:+5:8-()('&$.*),44520%)1.*153'&$+((+2+-&,%%%$1,,&)251-)53+()682:.;)260)/0*-(.(((+)'&%'%'%&*,*-0.*''%/'87871(&0.*0)66)61+&&&&%'&$&%%$.0*')-,/('%'&-86765&(4,)(&/-(%%*.($&&$'+43114.%%&''/-(%('&1-(.%%,'(,(1*%%,)%/./,(%)0/0).'-+./0)$%'$(%$)'1.4)*/'(,&)'(''%&').-,,5*.01+2/(&'+-(*&(((+,)/)1-444.-))-'(***.76(4(+*6/-.5.0+086,65521+646(%%'.,22%23.)&.,*,)'(%*(,+%14(1.2(,)%$&$+).'.,+$/*/161/-5.2)-%&0-22.$',-,.,221'3),+'*.1*33(3-&%&$%%1'.0*,36$4:=(()*+51693%/0''32(.4.,,-/*%&-%1()%&&**(16/*64*-/-(.*)*)&*+..//*0..2(&+(33(+'+)656+8799.*474)4776*.*+'2(/)*2=9146076'**&'(''&'+.+-351'3720)2/(.*'.*+$*,',*)$$&.0)%-5$/+.10,*887'444025)/4-(/'115+44+,,(')1),++(%%,+%&%&&&)+''&(+)/3/.%9741(-..+%,(&..,$2&222./1')(&&)).+3,.00+'('&&%+)/1-,)*-0,3112%-0'-,&(&(%')'*'',(&..1/*$.*,0,,.(%+*005/.4//2+,%&.5(9(*5998*45&'*)5.66577-))5(52*(%(%'$&2--(%,&,-.-4378<)*392.'98(/.%*9967;5*(0+,%+6748*1/,*,-()43*'877.(:48-).2/)+=<78*+942(0.(')789<8/.<=<):/89=('(''5+4*5;9;873590.*','((33./+00,''*'*(%',*+*%*(,%(&(%%&%(+,.++&*)$)0)-&-++/,*+*$*/30,%'+(()0/*01.,**+)(10,)-+*(%'-44+14.+1)00(*)&((+)&+/-(,-(&&%%(&+*-&'%'12%,++/'.)2//*.-'%(&$'&*.&0&+$%'-(,,,.)(%&.33/(+)/2/%(+%)**&$)40264/&416201/45313,171)640/313(40%-())+%&%&0584(7814*(%',51131*'&*0%-*'.1/5295'&(-*($)')++&,%$()638952337967(()(&&((/-%(&%%&(*&%''%(1/)&%,&'.45-633&2464552(749658951$.*+6'*,256)'+,(,22/42,'(58:-69,;8/*::<;0-+0,1(*%)+-6+%,0-*$&'3611066/8)--/:8)4,=,(<89+86-(0(%%/1,*621*3.+&%*.26/,2/*1317')&&*))./*6:5-4,94671'')*$&%&,-*%)42(1-)&)%&&%*,*,+'%,&%)'..37158263.6/5+)($&)')+&&/*,12.%+($674(,.'-(3*-.($(49++:9391(+'&''*.734)-0('1-.)&(+73('2,-=@9:9.2.+'(*-3(,6(:550-0'%++(,00)66)(.++84554'46554,'$)($,),&1-373(4'545.454-56)(,%,*&()85=85)96.85(0,86,*2+865*2632:88775')-+1.(,(&,%+'(,,'(-2//52340)*4-3)-63/'579558258/443*(/+(110&),--223/')*)%**$(%*+%%%&,.645*495/)()&)*&++(,)(&4'.11462*&&*64'/1-/0(7'5&4+/2-(9;<9992(0*)*)+.0,,1657676)3&&&)'$'*.$($&'(34.-0/**)%+%&,1,)'4'-5348;:0/%&&*22//'&,&0-.65-43%%+&-+&'&(/1,*&$%$($'$&$%'%',0+4344,---15751,&10)(2000'(32(,19811*)68:68;,081,,>87=6/4+7;579;)6)8+7))9460'.1/((,0/-1,)/52**04'34&0,2324'49+(:768'''3420)-*05'1,(%('&&&$+.&('(.)1735014'0*678866'499'0;9/):,242.&,/)/.1')((,4:987)&&%$(.44$+&&%&)17*-92;6*8?)(,030)--454&,&0-2'0145'/'255-&'%&315054)+(&%*(*+))#$%+*'4.703*75-45+.+37951(17/(84:8),*6.(&0/0'(886:7*3*0-&-''&*)%/1,-/%0)(144.46-11)-,-%*+.1.+('*&///--&)*./'103:6/2/%)%,-*%2873)-0,.'11%*',0,&/,+,()'*)),$%%00)%%%())++'0'372+320'+%*+%*'44)24-0'&&(14++*((%&16337&&&&44359'25-02.)%033.0/4'.34&'-(3+)43+3/,86;;50()),)-0,)(/*.684,/2*0531(570&+(952(3;89:=+6698*(13.-4)31&)%*(&&%(*-35197**5'(&).2*'0-)+0*)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0][0][2852:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d2e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
