# This workflow trains a model given a configuration, then basecalls using the 
# best model state using greedy and beam search, and finally, it evaluates
# the basecalls.
# The basecalling using greedy and beam and their evaluations can be done in
# parallel.
#
# To run: 
# snakemake --profile slurm --resources gpu_jobs=10 cpu_jobs=6 -n
# remove -n otherwise it is a dry run


import os
from snakemake.utils import min_version
min_version('6.13.1')


configfile:'config.yaml'

rule all:
    input:
        expand(
            os.path.join(
                config['model_dir'], 
                '{task}', 
                '{model}_{win}', 
                'evaluation_{ove}_{beam_size}_{beam_threshold}.csv',
            ),
            beam_size = config['beam_size'],
            beam_threshold = config['beam_threshold'],
            model = config['model'],
            win = config['window_size'],
            task = config['task'],
            ove = config['window_overlap'],
        )
        
rule evaluate:
    input:
        basecalls_path = os.path.join(
            config['model_dir'], 
            '{task}', '{cnn}_{enc}_{dec}_{con}_{win}', 
            'basecalls_{ove}_{beam_size}_{beam_threshold}.fastq',
        ),
        basecalling_done = os.path.join(
            config['model_dir'], 
            '{task}', 
            '{cnn}_{enc}_{dec}_{con}_{win}', 
            'basecalling_{ove}_{beam_size}_{beam_threshold}.done',
        )

    output:
        evaluation_file = os.path.join(
            config['model_dir'], 
            '{task}', 
            '{model}_{win}', 
            'evaluation_{ove}_{beam_size}_{beam_threshold}.csv',
        ),
        evaluation_done = touch(os.path.join(
            config['model_dir'], 
            '{task}', 
            '{model}_{win}', 
            'evaluation_{ove}_{beam_size}_{beam_threshold}.done'
        ))

    params:
        references_path = config['references_path'],
        processes = config['processes']

    resources:
        job_name = lambda wildcards: 'evaluate_' + wildcards.model_name,
        time = '12:00:00',
        mem_mb = str(config['processes']*2)+'G',
        cpus = config['processes'],
        partition = 'cpu',
        gpus = '0',
        cpu_jobs = 1


    shell:
        ' set +eu '
        ' && PS1=dummy ' 
        ' && . $(conda info --base)/etc/profile.d/conda.sh '
        ' && conda activate babe '
        ' && echo $CONDA_PREFIX; ' 
        ' python /hpc/compgen/users/mpages/babe/scripts/benchmark/evaluate_basecalls.py '\
        ' --output-file {output.evaluation_file} '\
        ' --basecalls-path {input.basecalls_path} '\
        ' --references-path {params.references_path} '\
        ' --processes {params.processes}; '


rule basecall:
    input:
        training_done=os.path.join(
            config['model_dir'], 
            '{task}', 
            '{model}_{win}', 
            'training.done',
        )

    output:
        basecalls_path = os.path.join(
            config['model_dir'], 
            '{task}', 
            '{model}_{win}', 
            'basecalls_{ove}_{beam_size}_{beam_threshold}.fastq',
        ),
        basecalling_done = touch(os.path.join(
            config['model_dir'], 
            '{task}', 
            '{model}_{win}', 
            'basecalling_{ove}_{beam_size}_{beam_threshold}.done',
        ))
    
    params:
        model_name = lambda wildcards: wildcards.model,
        window_overlap = lambda wildcards: wildcards.ove,
        file_list = config['file_list'],
        beam_size = lambda wildcards: wildcards.beam_size,
        beam_threshold = lambda wildcards: wildcards.beam_threshold,
        window_size = lambda wildcards: wildcards.win,
        batch_size = config["batch_size"],
        task = lambda wildcards: wildcards.task

    resources:
        job_name = lambda wildcards: 'basecall_' + wildcards.model,
        time = '6:00:00',
        mem_mb = '64G',
        cpus = 4,
        partition = 'gpu',
        gpus = 'RTX6000:1',
        gpu_jobs = 1

    shell:
        ' set +eu '
        ' && PS1=dummy ' 
        ' && . $(conda info --base)/etc/profile.d/conda.sh '
        ' && conda activate babe '
        ' && echo $CONDA_PREFIX; ' 
        ' export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64; '
        ' python /hpc/compgen/users/mpages/babe/scripts/benchmark/basecall_original.py '\
        ' --model-name {params.model_name} '\
        ' --output-file {output.basecalls_path} '\
        ' --task {params.task}'\
        ' --window-overlap {params.window_overlap} '\
        ' --file-list {params.file_list}'\
        ' --batch-size {params.batch_size} '\
        ' --beam-size {params.beam_size} '\
        ' --beam-threshold {params.beam_threshold} '\
        ' --chunk-size {params.window_size}; '


rule train:
    input:
        data_dir=os.path.join(config['data_dir'], '{task}', '{win}.0'),
        output_dir=config['model_dir']

    output:
        training_done=touch(os.path.join(
            config['model_dir'], 
            '{task}', 
            '{model}_{win}', 
            'training.done',
        ))
    
    params:
        model = lambda wildcards: wildcards.model,
        window_size = lambda wildcards: wildcards.win,
        task = lambda wildcards: wildcards.task,
        batch_size = config["batch_size"],
        use_scaler = config["use_scaler"]
    
    resources:
        job_name = lambda wildcards: 'train_' + wildcards.model,
        time = '5-00:00:00',
        mem_mb = '64G',
        cpus = 4,
        partition = 'gpu',
        gpus = 'RTX6000:1',
        gpu_jobs = 1

    shell:
        ' set +eu '
        ' && PS1=dummy ' 
        ' && . $(conda info --base)/etc/profile.d/conda.sh '
        ' && conda activate babe '
        ' && echo $CONDA_PREFIX; ' 
        ' export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64; '
        ' python /hpc/compgen/users/mpages/babe/scripts/benchmark/train_original.py '\
        ' --output-dir {input.output_dir} '\
        ' --data-dir {input.data_dir} '\
        ' --model {params.model} '\
        ' --window-size {params.window_size} '\
        ' --batch-size {params.batch_size} '\
        ' --task {params.task} '\
        ' {params.use_connector} '\
        ' {params.use_scaler}; '